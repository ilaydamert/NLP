from sklearn.feature_extraction.text import TfidfVectorizerimport pandas as pdimport reimport nltkimport stringfrom nltk.corpus import stopwordsfrom bs4 import BeautifulSoupfrom nltk.stem import WordNetLemmatizerdf = pd.read_csv("spam.csv", encoding="ISO-8859-1")df = df.loc[:, ~df.columns.str.contains('^Unnamed')]df = df.dropna(subset=["v2"])print(df.head())documents = df["v2"]labels = df["v1"]nltk.download("punkt")nltk.download("stopwords")nltk.download("wordnet")stop_words = set(stopwords.words("english"))def preprocessing(text):    if not text or not isinstance(text, str):        return ""    text = BeautifulSoup(text, "lxml").get_text()    text = " ".join(text.split())    text = text.lower()    text = text.translate(str.maketrans("", "", string.punctuation))    text = re.sub(r"[^A-Za-z0-9\s]", "", text)    text = " ".join([word for word in text.split() if len(word) > 2])    tokens = nltk.word_tokenize(text)    lemmatizer = WordNetLemmatizer()    lemmatized = [lemmatizer.lemmatize(w, pos="v") for w in tokens]    text = [word for word in lemmatized if word not in stop_words]    return " ".join(text)processed_documents = [preprocessing(doc) for doc in documents]vectorizer = TfidfVectorizer()X = vectorizer.fit_transform(processed_documents)feature_names = vectorizer.get_feature_names_out()tfidf_score = X.mean(axis=0).A1df_tfidf = pd.DataFrame({"word": feature_names, "tfidf_score": tfidf_score})df_tfid_sorted = df_tfidf.sort_values(by="tfidf_score", ascending=False)print(df_tfid_sorted.head())